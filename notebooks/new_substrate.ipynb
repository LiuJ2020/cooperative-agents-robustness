{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 16:13:19.506458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740759199.582176 1086492 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740759199.635391 1086492 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from meltingpot.utils import substrates\n",
    "from meltingpot.configs import scenarios, bots\n",
    "from meltingpot import substrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = scenarios.SCENARIO_CONFIGS[\"commons_harvest__open_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "substrate_name = scenario.substrate\n",
    "roles = scenario.roles\n",
    "substrate = substrate.build(substrate_name, roles = roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 16:13:24,994\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-02-28 16:13:25,096\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import air\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.rllib.policy import policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Mapping\n",
    "\n",
    "import dm_env\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PLAYER_STR_FORMAT = 'player_{index}'\n",
    "_WORLD_PREFIX = 'WORLD.'\n",
    "\n",
    "\n",
    "def timestep_to_observations(timestep: dm_env.TimeStep) -> Mapping[str, Any]:\n",
    "  gym_observations = {}\n",
    "  for index, observation in enumerate(timestep.observation):\n",
    "    gym_observations[PLAYER_STR_FORMAT.format(index=index)] = {\n",
    "        key: value\n",
    "        for key, value in observation.items()\n",
    "        if _WORLD_PREFIX not in key\n",
    "    }\n",
    "  return gym_observations\n",
    "\n",
    "\n",
    "def remove_world_observations_from_space(\n",
    "    observation: spaces.Dict) -> spaces.Dict:\n",
    "  return spaces.Dict({\n",
    "      key: observation[key] for key in observation if _WORLD_PREFIX not in key\n",
    "  })\n",
    "\n",
    "\n",
    "def spec_to_space(spec: tree.Structure[dm_env.specs.Array]) -> spaces.Space:\n",
    "  \"\"\"Converts a dm_env nested structure of specs to a Gym Space.\n",
    "\n",
    "  BoundedArray is converted to Box Gym spaces. DiscreteArray is converted to\n",
    "  Discrete Gym spaces. Using Tuple and Dict spaces recursively as needed.\n",
    "\n",
    "  Args:\n",
    "    spec: The nested structure of specs\n",
    "\n",
    "  Returns:\n",
    "    The Gym space corresponding to the given spec.\n",
    "  \"\"\"\n",
    "  if isinstance(spec, dm_env.specs.DiscreteArray):\n",
    "    return spaces.Discrete(spec.num_values)\n",
    "  elif isinstance(spec, dm_env.specs.BoundedArray):\n",
    "    return spaces.Box(spec.minimum, spec.maximum, spec.shape, spec.dtype)\n",
    "  elif isinstance(spec, dm_env.specs.Array):\n",
    "    if np.issubdtype(spec.dtype, np.floating):\n",
    "      return spaces.Box(-np.inf, np.inf, spec.shape, spec.dtype)\n",
    "    elif np.issubdtype(spec.dtype, np.integer):\n",
    "      info = np.iinfo(spec.dtype)\n",
    "      return spaces.Box(info.min, info.max, spec.shape, spec.dtype)\n",
    "    else:\n",
    "      raise NotImplementedError(f'Unsupported dtype {spec.dtype}')\n",
    "  elif isinstance(spec, (list, tuple)):\n",
    "    return spaces.Tuple([spec_to_space(s) for s in spec])\n",
    "  elif isinstance(spec, dict):\n",
    "    return spaces.Dict({key: spec_to_space(s) for key, s in spec.items()})\n",
    "  else:\n",
    "    raise ValueError('Unexpected spec of type {}: {}'.format(type(spec), spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import dm_env\n",
    "import dmlab2d\n",
    "from gymnasium import spaces\n",
    "from meltingpot import substrate\n",
    "from meltingpot.utils.policies import policy\n",
    "from ml_collections import config_dict\n",
    "import numpy as np\n",
    "from ray.rllib import algorithms\n",
    "from ray.rllib.env import multi_agent_env\n",
    "from ray.rllib.policy import sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PLAYER_STR_FORMAT = 'player_{index}'\n",
    "\n",
    "\n",
    "class MeltingPotEnv(multi_agent_env.MultiAgentEnv):\n",
    "  \"\"\"An adapter between the Melting Pot substrates and RLLib MultiAgentEnv.\"\"\"\n",
    "\n",
    "  def __init__(self, env: dmlab2d.Environment):\n",
    "    \"\"\"Initializes the instance.\n",
    "\n",
    "    Args:\n",
    "      env: dmlab2d environment to wrap. Will be closed when this wrapper closes.\n",
    "    \"\"\"\n",
    "    self._env = env\n",
    "    self._num_players = len(self._env.observation_spec())\n",
    "    self._ordered_agent_ids = [\n",
    "        PLAYER_STR_FORMAT.format(index=index)\n",
    "        for index in range(self._num_players)\n",
    "    ]\n",
    "    # RLLib requires environments to have the following member variables:\n",
    "    # observation_space, action_space, and _agent_ids\n",
    "    self._agent_ids = set(self._ordered_agent_ids)\n",
    "    # RLLib expects a dictionary of agent_id to observation or action,\n",
    "    # Melting Pot uses a tuple, so we convert\n",
    "    self.observation_space = self._convert_spaces_tuple_to_dict(\n",
    "        spec_to_space(self._env.observation_spec()),\n",
    "        remove_world_observations=True)\n",
    "    self.action_space = self._convert_spaces_tuple_to_dict(\n",
    "        spec_to_space(self._env.action_spec()))\n",
    "    super().__init__()\n",
    "\n",
    "  def reset(self, *args, **kwargs):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    timestep = self._env.reset()\n",
    "    return timestep_to_observations(timestep), {}\n",
    "\n",
    "  def step(self, action_dict):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    actions = [action_dict[agent_id] for agent_id in self._ordered_agent_ids]\n",
    "    timestep = self._env.step(actions)\n",
    "    rewards = {\n",
    "        agent_id: timestep.reward[index]\n",
    "        for index, agent_id in enumerate(self._ordered_agent_ids)\n",
    "    }\n",
    "    done = {'__all__': timestep.last()}\n",
    "    info = {}\n",
    "\n",
    "    observations = timestep_to_observations(timestep)\n",
    "    return observations, rewards, done, done, info\n",
    "\n",
    "  def close(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    self._env.close()\n",
    "\n",
    "  def get_dmlab2d_env(self):\n",
    "    \"\"\"Returns the underlying DM Lab2D environment.\"\"\"\n",
    "    return self._env\n",
    "\n",
    "  # Metadata is required by the gym `Env` class that we are extending, to show\n",
    "  # which modes the `render` method supports.\n",
    "  metadata = {'render.modes': ['rgb_array']}\n",
    "\n",
    "  def render(self) -> np.ndarray:\n",
    "    \"\"\"Render the environment.\n",
    "\n",
    "    This allows you to set `record_env` in your training config, to record\n",
    "    videos of gameplay.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: This returns a numpy.ndarray with shape (x, y, 3),\n",
    "        representing RGB values for an x-by-y pixel image, suitable for turning\n",
    "        into a video.\n",
    "    \"\"\"\n",
    "    observation = self._env.observation()\n",
    "    world_rgb = observation[0]['WORLD.RGB']\n",
    "\n",
    "    # RGB mode is used for recording videos\n",
    "    return world_rgb\n",
    "\n",
    "  def _convert_spaces_tuple_to_dict(\n",
    "      self,\n",
    "      input_tuple: spaces.Tuple,\n",
    "      remove_world_observations: bool = False) -> spaces.Dict:\n",
    "    \"\"\"Returns spaces tuple converted to a dictionary.\n",
    "\n",
    "    Args:\n",
    "      input_tuple: tuple to convert.\n",
    "      remove_world_observations: If True will remove non-player observations.\n",
    "    \"\"\"\n",
    "    return spaces.Dict({\n",
    "        agent_id: (remove_world_observations_from_space(input_tuple[i])\n",
    "                   if remove_world_observations else input_tuple[i])\n",
    "        for i, agent_id in enumerate(self._ordered_agent_ids)\n",
    "    })\n",
    "\n",
    "\n",
    "def env_creator(env_config):\n",
    "  \"\"\"Outputs an environment for registering.\"\"\"\n",
    "  env_config = config_dict.ConfigDict(env_config)\n",
    "  env = substrate.build(env_config['substrate'], roles=env_config['roles'])\n",
    "  env = MeltingPotEnv(env)\n",
    "  return env\n",
    "\n",
    "\n",
    "class RayModelPolicy(policy.Policy[policy.State]):\n",
    "  \"\"\"Policy wrapping an RLLib model for inference.\n",
    "\n",
    "  Note: Currently only supports a single input, batching is not enabled\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               model: algorithms.Algorithm,\n",
    "               policy_id: str = sample_batch.DEFAULT_POLICY_ID) -> None:\n",
    "    \"\"\"Initialize a policy instance.\n",
    "\n",
    "    Args:\n",
    "      model: An rllib.trainer.Trainer checkpoint.\n",
    "      policy_id: Which policy to use (if trained in multi_agent mode)\n",
    "    \"\"\"\n",
    "    self._model = model\n",
    "    self._prev_action = 0\n",
    "    self._policy_id = policy_id\n",
    "\n",
    "  def step(self, timestep: dm_env.TimeStep,\n",
    "           prev_state: policy.State) -> Tuple[int, policy.State]:\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    observations = {\n",
    "        key: value\n",
    "        for key, value in timestep.observation.items()\n",
    "        if 'WORLD' not in key\n",
    "    }\n",
    "\n",
    "    action, state, _ = self._model.compute_single_action(\n",
    "        observations,\n",
    "        prev_state,\n",
    "        policy_id=self._policy_id,\n",
    "        prev_action=self._prev_action,\n",
    "        prev_reward=timestep.reward)\n",
    "\n",
    "    self._prev_action = action\n",
    "    return action, state\n",
    "\n",
    "  def initial_state(self) -> policy.State:\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    self._prev_action = 0\n",
    "    return self._model.get_policy(self._policy_id).get_initial_state()\n",
    "\n",
    "  def close(self) -> None:\n",
    "    \"\"\"See base class.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_config(\n",
    "    substrate_name: str = \"bach_or_stravinsky_in_the_matrix__repeated\",\n",
    "    num_rollout_workers: int = 2,\n",
    "    rollout_fragment_length: int = 100,\n",
    "    train_batch_size: int = 6400,\n",
    "    fcnet_hiddens=(64, 64),\n",
    "    post_fcnet_hiddens=(256,),\n",
    "    lstm_cell_size: int = 256,\n",
    "    sgd_minibatch_size: int = 128,\n",
    "):\n",
    "  \"\"\"Get the configuration for running an agent on a substrate using RLLib.\n",
    "\n",
    "  We need the following 2 pieces to run the training:\n",
    "\n",
    "  Args:\n",
    "    substrate_name: The name of the MeltingPot substrate, coming from\n",
    "      `substrate.AVAILABLE_SUBSTRATES`.\n",
    "    num_rollout_workers: The number of workers for playing games.\n",
    "    rollout_fragment_length: Unroll time for learning.\n",
    "    train_batch_size: Batch size (batch * rollout_fragment_length)\n",
    "    fcnet_hiddens: Fully connected layers.\n",
    "    post_fcnet_hiddens: Layer sizes after the fully connected torso.\n",
    "    lstm_cell_size: Size of the LSTM.\n",
    "    sgd_minibatch_size: Size of the mini-batch for learning.\n",
    "\n",
    "  Returns:\n",
    "    The configuration for running the experiment.\n",
    "  \"\"\"\n",
    "  # Gets the default training configuration\n",
    "  config = ppo.PPOConfig()\n",
    "  # Number of arenas.\n",
    "  config.num_env_runners = num_rollout_workers\n",
    "  # This is to match our unroll lengths.\n",
    "  config.rollout_fragment_length = rollout_fragment_length\n",
    "  # Total (time x batch) timesteps on the learning update.\n",
    "  config.train_batch_size = train_batch_size\n",
    "  # Mini-batch size.\n",
    "  config.sgd_minibatch_size = sgd_minibatch_size\n",
    "  # Use the raw observations/actions as defined by the environment.\n",
    "  config.preprocessor_pref = None\n",
    "  # Use TensorFlow as the tensor framework.\n",
    "  config = config.framework(\"torch\")\n",
    "  # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "  config.num_gpus = int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\"))\n",
    "  config.log_level = \"DEBUG\"\n",
    "\n",
    "  # 2. Set environment config. This will be passed to\n",
    "  # the env_creator function via the register env lambda below.\n",
    "  player_roles = substrate.get_config(substrate_name).default_player_roles\n",
    "  config.env_config = {\"substrate\": substrate_name, \"roles\": player_roles}\n",
    "\n",
    "  config.env = \"meltingpot\"\n",
    "\n",
    "  # 4. Extract space dimensions\n",
    "  test_env = env_creator(config.env_config)\n",
    "\n",
    "  # Setup PPO with policies, one per entry in default player roles.\n",
    "  policies = {}\n",
    "  player_to_agent = {}\n",
    "  for i in range(len(player_roles)):\n",
    "    rgb_shape = test_env.observation_space[f\"player_{i}\"][\"RGB\"].shape\n",
    "    sprite_x = rgb_shape[0] // 8\n",
    "    sprite_y = rgb_shape[1] // 8\n",
    "\n",
    "    # policies[f\"agent_{i}\"] = policy.Policy(\n",
    "    #     policy_class=None,  # use default policy\n",
    "    #     observation_space=test_env.observation_space[f\"player_{i}\"],\n",
    "    #     action_space=test_env.action_space[f\"player_{i}\"],\n",
    "    #     config={\n",
    "    #         \"model\": {\n",
    "    #             \"conv_filters\": [[16, [8, 8], 8],\n",
    "    #                              [128, [sprite_x, sprite_y], 1]],\n",
    "    #         },\n",
    "    #     })\n",
    "    policies[f\"agent_{i}\"] = (\n",
    "        None,  # use default policy\n",
    "        test_env.observation_space[f\"player_{i}\"],\n",
    "        test_env.action_space[f\"player_{i}\"],\n",
    "        {\n",
    "            \"model\": {\n",
    "                \"conv_filters\": [[16, [8, 8], 8],\n",
    "                                 [128, [sprite_x, sprite_y], 1]],\n",
    "            },\n",
    "        })\n",
    "    \n",
    "    \n",
    "    player_to_agent[f\"player_{i}\"] = f\"agent_{i}\"\n",
    "\n",
    "  def policy_mapping_fn(agent_id, **kwargs):\n",
    "    del kwargs\n",
    "    return player_to_agent[agent_id]\n",
    "\n",
    "  # 5. Configuration for multi-agent setup with one policy per role:\n",
    "  config.multi_agent(policies=policies, policy_mapping_fn=policy_mapping_fn)\n",
    "\n",
    "  # 6. Set the agent architecture.\n",
    "  # Definition of the model architecture.\n",
    "  # The strides of the first convolutional layer were chosen to perfectly line\n",
    "  # up with the sprites, which are 8x8.\n",
    "  # The final layer must be chosen specifically so that its output is\n",
    "  # [B, 1, 1, X]. See the explanation in\n",
    "  # https://docs.ray.io/en/latest/rllib-models.html#built-in-models. It is\n",
    "  # because rllib is unable to flatten to a vector otherwise.\n",
    "  # The acb models used as baselines in the meltingpot paper were not run using\n",
    "  # rllib, so they used a different configuration for the second convolutional\n",
    "  # layer. It was 32 channels, [4, 4] kernel shape, and stride = 1.\n",
    "  config.model[\"fcnet_hiddens\"] = fcnet_hiddens\n",
    "  config.model[\"fcnet_activation\"] = \"relu\"\n",
    "  config.model[\"conv_activation\"] = \"relu\"\n",
    "  config.model[\"post_fcnet_hiddens\"] = post_fcnet_hiddens\n",
    "  config.model[\"post_fcnet_activation\"] = \"relu\"\n",
    "  config.model[\"use_lstm\"] = True\n",
    "  config.model[\"lstm_use_prev_action\"] = True\n",
    "  config.model[\"lstm_use_prev_reward\"] = False\n",
    "  config.model[\"lstm_cell_size\"] = lstm_cell_size\n",
    "\n",
    "  return config\n",
    "\n",
    "\n",
    "def train(config, num_iterations=1):\n",
    "  \"\"\"Trains a model.\n",
    "\n",
    "  Args:\n",
    "    config: model config\n",
    "    num_iterations: number of iterations ot train for.\n",
    "\n",
    "  Returns:\n",
    "    Training results.\n",
    "  \"\"\"\n",
    "  tune.register_env(\"meltingpot\", env_creator)\n",
    "  ray.shutdown()\n",
    "  ray.init()\n",
    "  stop = {\n",
    "      \"training_iteration\": num_iterations,\n",
    "  }\n",
    "  return tune.Tuner(\n",
    "      \"PPO\",\n",
    "      param_space=config.to_dict(),\n",
    "      run_config=air.RunConfig(stop=stop, verbose=1),\n",
    "  ).fit()\n",
    "\n",
    "\n",
    "def main():\n",
    "  config = get_config()\n",
    "  results = train(config, num_iterations=1)\n",
    "  print(results)\n",
    "  assert results.num_errors == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-02-28 17:49:31</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:15.69        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.4/31.3 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/16 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_meltingpot_539f6_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-02-28_17-29-53_143425_1099153/artifacts/2025-02-28_17-49-15/PPO_2025-02-28_17-49-15/driver_artifacts/PPO_meltingpot_539f6_00000_0_2025-02-28_17-49-15/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_meltingpot_539f6_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 17:49:15,551\tWARNING algorithm_config.py:4732 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "2025-02-28 17:49:15,552\tWARNING algorithm_config.py:4732 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "\u001b[36m(pid=1105475)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=1105475)\u001b[0m E0000 00:00:1740764957.307562 1105475 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=1105475)\u001b[0m E0000 00:00:1740764957.315843 1105475 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=1105475)\u001b[0m /home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "\u001b[36m(pid=1105475)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m 2025-02-28 17:49:23,639\tWARNING algorithm_config.py:4703 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m 2025-02-28 17:49:23,641\tWARNING algorithm_config.py:4732 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "\u001b[36m(pid=1105553)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=1105553)\u001b[0m E0000 00:00:1740764964.879871 1105553 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=1105553)\u001b[0m E0000 00:00:1740764964.885701 1105553 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=1105553)\u001b[0m /home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "\u001b[36m(pid=1105553)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=1105552)\u001b[0m /home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "\u001b[36m(pid=1105552)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=1105553, ip=172.20.38.193, actor_id=f22cea63ae4706335232843a05000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7fcca77f0b10>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     env_spec = _find_spec(id)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m                ^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m gymnasium.error.NameNotFound: Environment `meltingpot` doesn't exist.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=1105553, ip=172.20.38.193, actor_id=f22cea63ae4706335232843a05000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7fcca77f0b10>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 100, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     self.make_env()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 654, in make_env\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     gym.make_vec(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     env = gym.vector.SyncVectorEnv(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m                  ^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     env = env_creator(**env_spec_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('meltingpot') is:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m a) Not a supported or -installed environment.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m Try one of the following:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m    Then in your config, do `config['env'] = [name]`.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105553)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[36m(pid=1105552)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=1105552)\u001b[0m E0000 00:00:1740764964.879789 1105552 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=1105552)\u001b[0m E0000 00:00:1740764964.885596 1105552 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105552)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105552)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=1105552)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m 2025-02-28 17:49:31,202\tERROR actor_manager.py:833 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=1105552, ip=172.20.38.193, actor_id=145ba2f78e4dd4b08db0dcdc05000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7facc43a1690>)\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`), taking actor 1 out of service.\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m 2025-02-28 17:49:31,203\tERROR actor_manager.py:833 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=1105553, ip=172.20.38.193, actor_id=f22cea63ae4706335232843a05000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7fcca77f0b10>)\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`), taking actor 2 out of service.\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m 2025-02-28 17:49:31,203\tERROR env_runner_group.py:689 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=1105552, ip=172.20.38.193, actor_id=145ba2f78e4dd4b08db0dcdc05000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7facc43a1690>)\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m 2025-02-28 17:49:31,203\tERROR env_runner_group.py:689 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=1105553, ip=172.20.38.193, actor_id=f22cea63ae4706335232843a05000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7fcca77f0b10>)\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "2025-02-28 17:49:31,224\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_meltingpot_539f6_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/_private/worker.py\", line 2771, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/_private/worker.py\", line 921, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=1105475, ip=172.20.38.193, actor_id=81d8c33dbb562e0be4bac2b105000000, repr=PPO(env=meltingpot; env-runners=2; learners=0; multi-agent=False))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "    env_spec = _find_spec(id)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment `meltingpot` doesn't exist.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=1105475, ip=172.20.38.193, actor_id=81d8c33dbb562e0be4bac2b105000000, repr=PPO(env=meltingpot; env-runners=2; learners=0; multi-agent=False))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 528, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 157, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 631, in setup\n",
      "    self.env_runner_group = EnvRunnerGroup(\n",
      "                            ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 198, in __init__\n",
      "    self._setup(\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 293, in _setup\n",
      "    self._local_env_runner = self._make_worker(\n",
      "                             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 1207, in _make_worker\n",
      "    return self.env_runner_cls(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 100, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 654, in make_env\n",
      "    gym.make_vec(\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "    env = gym.vector.SyncVectorEnv(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "    self.envs = [env_fn() for env_fn in env_fns]\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "    self.envs = [env_fn() for env_fn in env_fns]\n",
      "                 ^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "    single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "    env = env_creator(**env_spec_kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('meltingpot') is:\n",
      "a) Not a supported or -installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config['env'] = [name]`.\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "2025-02-28 17:49:31,233\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/jacobliu/ray_results/PPO_2025-02-28_17-49-15' in 0.0047s.\n",
      "2025-02-28 17:49:31,237\tERROR tune.py:1037 -- Trials did not complete: [PPO_meltingpot_539f6_00000]\n",
      "2025-02-28 17:49:31,238\tINFO tune.py:1041 -- Total run time: 15.71 seconds (15.68 seconds for the tuning loop).\n",
      "2025-02-28 17:49:31,242\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- PPO_meltingpot_539f6_00000: FileNotFoundError('Could not fetch metrics for PPO_meltingpot_539f6_00000: both result.json and progress.csv were not found at /home/jacobliu/ray_results/PPO_2025-02-28_17-49-15/PPO_meltingpot_539f6_00000_0_2025-02-28_17-49-15')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResultGrid<[\n",
      "  Result(\n",
      "    error='ActorDiedError',\n",
      "    metrics={},\n",
      "    path='/home/jacobliu/ray_results/PPO_2025-02-28_17-49-15/PPO_meltingpot_539f6_00000_0_2025-02-28_17-49-15',\n",
      "    filesystem='local',\n",
      "    checkpoint=None\n",
      "  )\n",
      "]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=1105475)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=1105475, ip=172.20.38.193, actor_id=81d8c33dbb562e0be4bac2b105000000, repr=PPO(env=meltingpot; env-runners=2; learners=0; multi-agent=False))\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \u001b[36mray::PPO.__init__()\u001b[39m (pid=1105475, ip=172.20.38.193, actor_id=81d8c33dbb562e0be4bac2b105000000, repr=PPO(env=meltingpot; env-runners=2; learners=0; multi-agent=False))\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m     super().__init__(\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 631, in setup\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m     self.env_runner_group = EnvRunnerGroup(\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m                             ^^^^^^^^^^^^^^^\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m     self._setup(\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 293, in _setup\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m     self._local_env_runner = self._make_worker(\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m                              ^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m   File \"/home/jacobliu/miniconda3/envs/cs234_finalproject/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py\", line 1207, in _make_worker\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m     return self.env_runner_cls(**kwargs)\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(PPO pid=1105475)\u001b[0m \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[34], line 147\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m results \u001b[38;5;241m=\u001b[39m train(config, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m results\u001b[38;5;241m.\u001b[39mnum_errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs234_finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
